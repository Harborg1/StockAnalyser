
# import yfinance as yf
# import matplotlib.pyplot as plt
# import pandas as pd
# import numpy as np
# import statsmodels.api as sm
# import seaborn as sns
# from sklearn.metrics import confusion_matrix, roc_curve, auc
# from sklearn.model_selection import train_test_split
# from sklearn.model_selection import TimeSeriesSplit
# from sklearn.linear_model import LogisticRegression
# from sklearn.preprocessing import StandardScaler

# TICKER = 'SPY'

# INTERVAL = '1h'

# if INTERVAL == '1h':
#     PERIOD = '730d'
# else:

#     PERIOD = 'max'

# SHIFT = 55
# MACD_FAST = 12
# MACD_SLOW = 27
# MACD_SPAN = 9
# MFI_LENGTH = 14
# MFI_OVERBOUGHT = 70
# MFI_OVERSOLD = 30
# RSI_LENGTH = 14
# RSI_OVERBOUGHT = 7
# RSI_OVERSOLD = 30
# BB_LEN = 20
# DEVS = 2
# LOOKBACK = 10000

# STRATEGY = ['Volume', 'Open', 'High', 'Low', 'Close', 'MACD_hist', 'MFI', 'BB', 'RSI']

# def get_data(ticker=TICKER, lookback=LOOKBACK, interval=INTERVAL):
#     df = yf.download(ticker, interval=interval, auto_adjust=False, period=PERIOD)
#     df.columns = df.columns.get_level_values(0)
#     df = df.reset_index()
#     df = df.loc[:, ~df.columns.duplicated()]
#     return df.iloc[-lookback:, :]

# def add_MACD(df, fast=MACD_FAST, slow=MACD_SLOW, span=MACD_SPAN):
#     df[f'{fast}_ema'] = df['Close'].ewm(span=fast).mean()
#     df[f'{slow}_ema'] = df['Close'].ewm(span=slow).mean()
#     df['MACD'] = df[f'{fast}_ema'] - df[f'{slow}_ema']
#     df['Signal'] = df['MACD'].ewm(span=span).mean()
#     df['MACD_hist'] = df['MACD'] - df['Signal']
#     return df

# def add_MFI(df, length=MFI_LENGTH, overbought=MFI_OVERBOUGHT, oversold=MFI_OVERSOLD):
#     df = df.copy()
#     df['Typical_Price'] = (df['High'] + df['Low'] + df['Close']) / 3
#     df['Raw_Money_Flow'] = df['Typical_Price'] * df['Volume']
#     df['Price_Change'] = df['Typical_Price'].diff()
#     df['Pos_Flow'] = np.where(df['Price_Change'] > 0, df['Raw_Money_Flow'], 0)
#     df['Neg_Flow'] = np.where(df['Price_Change'] < 0, df['Raw_Money_Flow'], 0)
#     pos_sum = df['Pos_Flow'].rolling(window=length).sum()
#     neg_sum = df['Neg_Flow'].rolling(window=length).sum()
#     mfr = pos_sum / neg_sum
#     df['MFI'] = 100 - (100 / (1 + mfr))
#     return df.dropna()

# def add_BB(df, devs=DEVS, bb_len=BB_LEN):
#     df['BB_SMA'] = df['Close'].rolling(bb_len).mean()
#     df['BB_STD'] = df['Close'].rolling(bb_len).std()
#     df['Upper_Band'] = df['BB_SMA'] + (devs * df['BB_STD'])
#     df['Lower_Band'] = df['BB_SMA'] - (devs * df['BB_STD'])
#     df['BB'] = (df['Upper_Band'] - df['Close']) / (df['Upper_Band'] - df['Lower_Band'])
#     return df.dropna()

# def add_RSI(df, length=RSI_LENGTH, overbought=RSI_OVERBOUGHT, oversold=RSI_OVERSOLD):
#     price_change = df['Close'].diff()
#     gain = price_change.where(price_change > 0, 0)
#     loss = -price_change.where(price_change < 0, 0)
#     avg_gain = gain.rolling(window=length).mean()
#     avg_loss = loss.rolling(window=length).mean()
#     rs = avg_gain / avg_loss
#     df['RSI'] = 100 - (100 / (1 + rs))
#     return df.dropna()

# def add_target(df, shift=SHIFT):
#     df[f'Close + {shift}'] = df['Close'].shift(-shift)
#     df['Target'] = (df[f'Close + {shift}'] > df['Close']) * 1
#     return df


# def generate_logistic_regression_output(df, features=STRATEGY, target='Target', splits=5):
#     subset = df[features + [target, 'Datetime', 'Close']].dropna()
#     dates = subset['Datetime']
#     closes = subset['Close']
#     X = subset[features]
#     y = subset[target]

#     scaler = StandardScaler()
#     X_scaled = scaler.fit_transform(X)

#     tscv = TimeSeriesSplit(n_splits=splits)
#     all_results = []

#     for _, (train_idx, test_idx) in enumerate(tscv.split(X_scaled)):
#         X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]
#         y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
#         close_test = closes.iloc[test_idx]
#         date_test = dates.iloc[test_idx]

#         model = LogisticRegression(penalty='l2', C=0.1, solver='liblinear', class_weight='balanced')
#         model.fit(X_train, y_train)
#         y_pred_prob = model.predict_proba(X_test)[:, 1]
#         y_pred = (y_pred_prob > 0.6).astype(int)

#         if y_test.nunique() < 2:
#             continue

#          # Ensure all arrays are converted to the same length before building the DataFrame
#         date_array = np.array(date_test).reshape(-1)
#         close_array = np.array(close_test).reshape(-1)
#         target_array = np.array(y_test).reshape(-1)
#         pred_array = np.array(y_pred).reshape(-1)
#         prob_array = np.array(y_pred_prob).reshape(-1)

#         min_len = min(len(date_array), len(close_array), len(target_array), len(pred_array), len(prob_array))

#         df_out = pd.DataFrame({
#             'Datetime': date_array[:min_len],
#             'Close': close_array[:min_len],
#             'Target': target_array[:min_len],
#             'Prediction': pred_array[:min_len],
#             'Probability': prob_array[:min_len]
#         })

#         all_results.append(df_out)

#     test_df = pd.concat(all_results).reset_index(drop=True)

#     plt.figure()
#     plt.hist(test_df['Probability'], bins=50)
#     plt.title('Distribution of Logistic Predictions')
#     plt.tight_layout()
#     plt.show()

#     return test_df, test_df['Target'], test_df['Probability']



# def analyze_logistic_regresison(df, y_true, y_scores, title=f'{STRATEGY} ROC Plot'):
#     cm = confusion_matrix(df['Target'], df['Prediction'])
#     labels = ['Down (0)', 'Up (1)']

#     cm_df = pd.DataFrame(cm, index=labels, columns=labels)
#     plt.figure()
#     sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='.0f')
#     plt.title("Confusion Matrix")
#     plt.xlabel("Predicted")
#     plt.ylabel("Actual")
#     plt.tight_layout()
#     plt.show()

  

#     fpr, tpr, thresholds = roc_curve(y_true, y_scores)
#     roc_auc = auc(fpr, tpr)

#     plt.figure()
#     plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
#     plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing')
#     plt.xlabel('False Positive Rate')
#     plt.ylabel('True Positive Rate')
#     plt.title(title)
#     plt.legend(loc="lower right")
#     plt.grid(True)
#     plt.tight_layout()
#     plt.show()

#     return df

# def build_feature_set(df, shift):
#     df = df.copy()

#     # Step 1: Compute indicators using only past data up to time t
#     df = add_MACD(df)
#     df = add_MFI(df)
#     df = add_BB(df)
#     df = add_RSI(df)

#     df = df.dropna().reset_index(drop=True)

#     # Step 2: Add future target first, before shifting features
#     df['Target'] = (df['Close'].shift(-shift) > df['Close']).astype(int)

#     # Step 3: Shift features forward by SHIFT steps to use only info available at time t
#     for col in STRATEGY:
#         if col in df.columns:
#             df[col] = df[col].shift(shift)

#     # Step 4: Drop all rows with incomplete features or target
#     df = df.dropna(subset=STRATEGY + ['Target']).reset_index(drop=True)

#     return df




# def find_optimal_shift(df, shift_range, preprocessed=False):
  

#     results = []
#     for shift in shift_range:
#         shift_val, auc_score = evaluate_shift(df, shift)
#         print(f"SHIFT={shift_val} -> AUC={auc_score:.4f}")
#         results.append((shift_val, auc_score))

#     results_df = pd.DataFrame(results, columns=['SHIFT', 'AUC'])
#     best_shift = results_df.loc[results_df['AUC'].idxmax()]

#     plt.figure()
#     plt.plot(results_df['SHIFT'], results_df['AUC'], marker='o')
#     plt.title("AUC vs SHIFT")
#     plt.xlabel("SHIFT (Future Prediction Horizon)")
#     plt.ylabel("AUC")
#     plt.grid(True)
#     plt.tight_layout()
#     plt.show()
#     print(f"Optimal SHIFT = {best_shift['SHIFT']} with AUC = {best_shift['AUC']:.4f}")
#     return int(best_shift['SHIFT'])



# def backtest_strategy(df, shift, threshold=0.6, original_df=None):
#     df = df.copy()
#     df = df.dropna(subset=['Prediction', 'Close'])

#     df['Buy_Signal'] = df['Prediction'] > threshold
#     df['Entry_Price'] = df['Close']
#     df['Exit_Price'] = df['Close'].shift(-shift)
#     df['PnL'] = np.where(df['Buy_Signal'], df['Exit_Price'] - df['Entry_Price'], 0)

#     trades = df[df['Buy_Signal']].copy()
#     trades = trades.dropna(subset=['Exit_Price'])

#     if 'Datetime' in trades.columns:
#         trade_log = trades[['Datetime', 'Entry_Price', 'Exit_Price', 'PnL', 'Prediction']].copy()
#     else:
#         trade_log = trades[['Entry_Price', 'Exit_Price', 'PnL', 'Prediction']].copy()
#         trade_log['Datetime'] = pd.NaT

#     trade_log['Datetime'] = trade_log['Datetime'].astype(str)
#     trade_log.index.name = 'TradeIndex'

#     total_trades = len(trade_log)
#     winning_trades = (trade_log['PnL'] > 0).sum()
#     win_rate = winning_trades / total_trades if total_trades > 0 else 0
#     total_return = trade_log['PnL'].sum()

#     print(f"\nStrategy Backtest:")
#     print(f"Total Trades: {total_trades}")
#     print(f"Win Rate: {win_rate:.2%}")
#     print(f"Total Return: {total_return:.2f}")

#     if original_df is not None and 'Close' in original_df:
#         buy_hold_return = original_df['Close'].iloc[-1] - original_df['Close'].iloc[0]
#         print(f"Buy and Hold Return: {buy_hold_return:.2f} points")

#     trade_log.to_csv("trades_with_dates.csv", index=True)

#     return trade_log

# def evaluate_shift(df_raw, shift, features=STRATEGY, target='Target', test_size=0.3):
#     df = build_feature_set(df_raw.copy(), shift=shift)
#     subset = df[features + [target]].dropna()

#     if subset.empty or subset[target].nunique() < 2:
#         print(f"Skipping SHIFT={shift}: not enough data or only one class.")
#         return shift, float("nan")

#     X = subset[features]
#     y = subset[target]

#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=False)
#     X_train_const = sm.add_constant(X_train)
#     X_test_const = sm.add_constant(X_test)

#     try:
#         model = sm.Logit(y_train, X_train_const).fit(disp=0)
#         y_pred_prob = model.predict(X_test_const)
#         fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
#         roc_auc = auc(fpr, tpr)
#     except Exception as e:
#         print(f"Failed for SHIFT={shift}: {e}")
#         roc_auc = 0.0

#     return shift, roc_auc


# def find_optimal_shift(df_raw, shift_range):
#     results = []
#     for shift in shift_range:
#         shift_val, auc_score = evaluate_shift(df_raw, shift)
#         print(f"SHIFT={shift_val} -> AUC={auc_score:.4f}")
#         results.append((shift_val, auc_score))

#     results_df = pd.DataFrame(results, columns=['SHIFT', 'AUC'])
#     best_shift = results_df.loc[results_df['AUC'].idxmax()]

#     plt.figure()
#     plt.plot(results_df['SHIFT'], results_df['AUC'], marker='o')
#     plt.title("AUC vs SHIFT")
#     plt.xlabel("SHIFT (Future Prediction Horizon)")
#     plt.ylabel("AUC")
#     plt.grid(True)
#     plt.tight_layout()
#     plt.show()
#     print(f"Optimal SHIFT = {best_shift['SHIFT']} with AUC = {best_shift['AUC']:.4f}")
#     return int(best_shift['SHIFT'])


# def check_for_overfitting_with_shift_analysis(df_raw, train_ratio=0.8, shift_range=range(5, 150)):
#     split_idx = int(len(df_raw) * train_ratio)
#     raw_train = df_raw.iloc[:split_idx].copy()
#     raw_test = df_raw.iloc[split_idx:].copy()

#     print("\nEvaluating shift on TRAINING SET")
#     optimal_shift_train = find_optimal_shift(raw_train, shift_range)

#     print("\nEvaluating shift on TEST SET")
#     optimal_shift_test = find_optimal_shift(raw_test, shift_range)

#     print(f"\nTrain Optimal SHIFT: {optimal_shift_train}, Test Optimal SHIFT: {optimal_shift_test}")

# def main():
#     original_df = get_data()

#     split_idx = int(len(original_df) * 0.8)
#     raw_train = original_df.iloc[:split_idx].copy()
#     raw_test = original_df.iloc[split_idx:].copy()

#     train_df = build_feature_set(raw_train, shift=SHIFT)
#     test_df = build_feature_set(raw_test, shift=SHIFT)

#     model = LogisticRegression(penalty='l2', C=0.1, solver='liblinear', class_weight='balanced')
#     X_train = train_df[STRATEGY]
#     y_train = train_df['Target']
#     scaler = StandardScaler()
#     X_train_scaled = scaler.fit_transform(X_train)
#     model.fit(X_train_scaled, y_train)

#     X_test = test_df[STRATEGY]
#     y_test = test_df['Target']
#     X_test_scaled = scaler.transform(X_test)
#     y_pred_prob = model.predict_proba(X_test_scaled)[:, 1]
#     y_pred = (y_pred_prob > 0.6).astype(int)

#     result_df = test_df.copy()
#     result_df['Prediction'] = y_pred
#     result_df['Probability'] = y_pred_prob
#     result_df = result_df.loc[:, ~result_df.columns.duplicated()]

#     # === 🚨 Insert Diagnostic Plot Here ===
#     result_df['Future_Return'] = result_df['Close'].shift(-SHIFT) - result_df['Close']

#     plt.figure()
#     plt.scatter(result_df['Probability'], result_df['Future_Return'], alpha=0.3)
#     plt.axhline(0, color='grey', linestyle='--')
#     plt.xlabel("Model Predicted Probability")
#     plt.ylabel(f"Future {SHIFT}-Step Return")
#     plt.title("Probability vs Future Return")
#     plt.grid(True)
#     plt.tight_layout()
#     plt.show()

#     # === Then continue with backtesting ===
#     trades = backtest_strategy(result_df, shift=SHIFT, threshold=0.6, original_df=raw_test)

#     check_for_overfitting_with_shift_analysis(original_df)

#     return trades

# df = main()
